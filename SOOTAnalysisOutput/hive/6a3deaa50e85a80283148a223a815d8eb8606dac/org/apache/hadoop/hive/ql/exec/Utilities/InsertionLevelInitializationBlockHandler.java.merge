package org.apache.hadoop.hive.ql.exec;

import java.beans.DefaultPersistenceDelegate;
import java.beans.Encoder;
import java.beans.ExceptionListener;
import java.beans.Expression;
import java.beans.PersistenceDelegate;
import java.beans.Statement;
import java.beans.XMLDecoder;
import java.beans.XMLEncoder;
import java.io.BufferedReader;
import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.DataInput;
import java.io.EOFException;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.io.OutputStream;
import java.io.PrintStream;
import java.io.Serializable;
import java.io.UnsupportedEncodingException;
import java.net.URI;
import java.net.URL;
import java.net.URLClassLoader;
import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.sql.SQLTransientException;
import java.sql.Timestamp;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Calendar;
import java.util.Collection;
import java.util.Collections;
import java.util.Date;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.LinkedHashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Properties;
import java.util.Random;
import java.util.Set;
import java.util.UUID;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.Future;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import org.antlr.runtime.CommonToken;
import org.apache.commons.lang.StringUtils;
import org.apache.commons.lang.WordUtils;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.ContentSummary;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.PathFilter;
import org.apache.hadoop.hive.common.HiveInterruptCallback;
import org.apache.hadoop.hive.common.HiveInterruptUtils;
import org.apache.hadoop.hive.conf.HiveConf;
import org.apache.hadoop.hive.metastore.Warehouse;
import org.apache.hadoop.hive.metastore.api.FieldSchema;
import org.apache.hadoop.hive.metastore.api.Order;
import org.apache.hadoop.hive.ql.Context;
import org.apache.hadoop.hive.ql.Driver;
import org.apache.hadoop.hive.ql.ErrorMsg;
import org.apache.hadoop.hive.ql.QueryPlan;
import org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter;
import org.apache.hadoop.hive.ql.exec.mr.ExecDriver;
import org.apache.hadoop.hive.ql.exec.mr.ExecMapper;
import org.apache.hadoop.hive.ql.exec.mr.ExecReducer;
import org.apache.hadoop.hive.ql.exec.mr.MapRedTask;
import org.apache.hadoop.hive.ql.io.ContentSummaryInputFormat;
import org.apache.hadoop.hive.ql.io.HiveFileFormatUtils;
import org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat;
import org.apache.hadoop.hive.ql.io.HiveInputFormat;
import org.apache.hadoop.hive.ql.io.HiveOutputFormat;
import org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat;
import org.apache.hadoop.hive.ql.io.OneNullRowInputFormat;
import org.apache.hadoop.hive.ql.io.RCFile;
import org.apache.hadoop.hive.ql.io.ReworkMapredInputFormat;
import org.apache.hadoop.hive.ql.io.rcfile.merge.MergeWork;
import org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileMergeMapper;
import org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanMapper;
import org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanWork;
import org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateMapper;
import org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork;
import org.apache.hadoop.hive.ql.log.PerfLogger;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.metadata.Partition;
import org.apache.hadoop.hive.ql.metadata.Table;
import org.apache.hadoop.hive.ql.parse.SemanticException;
import org.apache.hadoop.hive.ql.plan.BaseWork;
import org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx;
import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
import org.apache.hadoop.hive.ql.plan.FileSinkDesc;
import org.apache.hadoop.hive.ql.plan.GroupByDesc;
import org.apache.hadoop.hive.ql.plan.MapWork;
import org.apache.hadoop.hive.ql.plan.MapredWork;
import org.apache.hadoop.hive.ql.plan.OperatorDesc;
import org.apache.hadoop.hive.ql.plan.PartitionDesc;
import org.apache.hadoop.hive.ql.plan.PlanUtils;
import org.apache.hadoop.hive.ql.plan.PlanUtils.ExpressionTypes;
import org.apache.hadoop.hive.ql.plan.ReduceWork;
import org.apache.hadoop.hive.ql.plan.TableDesc;
import org.apache.hadoop.hive.ql.plan.api.Adjacency;
import org.apache.hadoop.hive.ql.plan.api.Graph;
import org.apache.hadoop.hive.ql.session.SessionState;
import org.apache.hadoop.hive.ql.stats.StatsFactory;
import org.apache.hadoop.hive.ql.stats.StatsPublisher;
import org.apache.hadoop.hive.serde.serdeConstants;
import org.apache.hadoop.hive.serde2.SerDeException;
import org.apache.hadoop.hive.serde2.Serializer;
import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
import org.apache.hadoop.hive.shims.ShimLoader;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.SequenceFile;
import org.apache.hadoop.io.SequenceFile.CompressionType;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.DefaultCodec;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.InputFormat;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.mapred.SequenceFileInputFormat;
import org.apache.hadoop.mapred.SequenceFileOutputFormat;
import org.apache.hadoop.util.ReflectionUtils;
import org.apache.hadoop.util.Shell;
import com.esotericsoftware.kryo.Kryo;
import com.esotericsoftware.kryo.io.Input;
import com.esotericsoftware.kryo.io.Output;
import com.esotericsoftware.kryo.serializers.FieldSerializer;

@SuppressWarnings("nls")
public final class Utilities {

    public static String HADOOP_LOCAL_FS = "file:///";

    public static String MAP_PLAN_NAME = "map.xml";

    public static String REDUCE_PLAN_NAME = "reduce.xml";

    public static final String MAPRED_MAPPER_CLASS = "mapred.mapper.class";

    public static final String MAPRED_REDUCER_CLASS = "mapred.reducer.class";

    public static enum ReduceField {

        KEY, VALUE
    }

    public static List<String> reduceFieldNameList;

    static {
        reduceFieldNameList = new ArrayList<String>();
        for (ReduceField r : ReduceField.values()) {
            reduceFieldNameList.add(r.toString());
        }
    }

    private Utilities() {
    }

    private static Map<Path, BaseWork> gWorkMap = Collections.synchronizedMap(new HashMap<Path, BaseWork>());

    private static final String CLASS_NAME = Utilities.class.getName();

    private static final Log LOG = LogFactory.getLog(CLASS_NAME);

    public static void clearWork(Configuration conf) {
        Path mapPath = getPlanPath(conf, MAP_PLAN_NAME);
        Path reducePath = getPlanPath(conf, REDUCE_PLAN_NAME);
        if (mapPath == null || reducePath == null) {
            return;
        }
        try {
            FileSystem fs = mapPath.getFileSystem(conf);
            if (fs.exists(mapPath)) {
                fs.delete(mapPath, true);
            }
            if (fs.exists(reducePath)) {
                fs.delete(reducePath, true);
            }
        } catch (Exception e) {
            LOG.warn("Failed to clean-up tmp directories.", e);
        } finally {
            if (mapPath != null) {
                gWorkMap.remove(mapPath);
            }
            if (reducePath != null) {
                gWorkMap.remove(reducePath);
            }
        }
    }

    public static MapredWork getMapRedWork(Configuration conf) {
        MapredWork w = new MapredWork();
        w.setMapWork(getMapWork(conf));
        w.setReduceWork(getReduceWork(conf));
        return w;
    }

    public static MapWork getMapWork(Configuration conf) {
        return (MapWork) getBaseWork(conf, MAP_PLAN_NAME);
    }

    public static ReduceWork getReduceWork(Configuration conf) {
        return (ReduceWork) getBaseWork(conf, REDUCE_PLAN_NAME);
    }

    private static BaseWork getBaseWork(Configuration conf, String name) {
        BaseWork gWork = null;
        Path path = null;
        InputStream in = null;
        try {
            path = getPlanPath(conf, name);
            assert path != null;
            gWork = gWorkMap.get(path);
            if (gWork == null) {
                Path localPath;
                if (ShimLoader.getHadoopShims().isLocalMode(conf)) {
                    localPath = path;
                } else {
                    localPath = new Path(name);
                }
                in = new FileInputStream(localPath.toUri().getPath());
                if (MAP_PLAN_NAME.equals(name)) {
                    if (ExecMapper.class.getName().equals(conf.get(MAPRED_MAPPER_CLASS))) {
                        gWork = deserializePlan(in, MapWork.class, conf);
                    } else if (RCFileMergeMapper.class.getName().equals(conf.get(MAPRED_MAPPER_CLASS))) {
                        gWork = deserializePlan(in, MergeWork.class, conf);
                    } else if (ColumnTruncateMapper.class.getName().equals(conf.get(MAPRED_MAPPER_CLASS))) {
                        gWork = deserializePlan(in, ColumnTruncateWork.class, conf);
                    } else if (PartialScanMapper.class.getName().equals(conf.get(MAPRED_MAPPER_CLASS))) {
                        gWork = deserializePlan(in, PartialScanWork.class, conf);
                    } else {
                        throw new RuntimeException("unable to determine work from configuration ." + MAPRED_MAPPER_CLASS + " was " + conf.get(MAPRED_MAPPER_CLASS));
                    }
                } else if (REDUCE_PLAN_NAME.equals(name)) {
                    if (ExecReducer.class.getName().equals(conf.get(MAPRED_REDUCER_CLASS))) {
                        gWork = deserializePlan(in, ReduceWork.class, conf);
                    } else {
                        throw new RuntimeException("unable to determine work from configuration ." + MAPRED_REDUCER_CLASS + " was " + conf.get(MAPRED_REDUCER_CLASS));
                    }
                }
                gWorkMap.put(path, gWork);
            }
            return gWork;
        } catch (FileNotFoundException fnf) {
            LOG.debug("No plan file found: " + path);
            return null;
        } catch (Exception e) {
            LOG.error("Failed to load plan: " + path, e);
            throw new RuntimeException(e);
        } finally {
            if (in != null) {
                try {
                    in.close();
                } catch (IOException cantBlameMeForTrying) {
                }
            }
        }
    }

    public static void setWorkflowAdjacencies(Configuration conf, QueryPlan plan) {
        try {
            Graph stageGraph = plan.getQueryPlan().getStageGraph();
            if (stageGraph == null) {
                return;
            }
            List<Adjacency> adjList = stageGraph.getAdjacencyList();
            if (adjList == null) {
                return;
            }
            for (Adjacency adj : adjList) {
                List<String> children = adj.getChildren();
                if (children == null || children.isEmpty()) {
                    return;
                }
                conf.setStrings("mapreduce.workflow.adjacency." + adj.getNode(), children.toArray(new String[children.size()]));
            }
        } catch (IOException e) {
        }
    }

    public static List<String> getFieldSchemaString(List<FieldSchema> fl) {
        if (fl == null) {
            return null;
        }
        ArrayList<String> ret = new ArrayList<String>();
        for (FieldSchema f : fl) {
            ret.add(f.getName() + " " + f.getType() + (f.getComment() != null ? (" " + f.getComment()) : ""));
        }
        return ret;
    }

    public static class EnumDelegate extends DefaultPersistenceDelegate {

        @Override
        protected Expression instantiate(Object oldInstance, Encoder out) {
            return new Expression(Enum.class, "valueOf", new Object[] { oldInstance.getClass(), ((Enum<?>) oldInstance).name() });
        }

        @Override
        protected boolean mutatesTo(Object oldInstance, Object newInstance) {
            return oldInstance == newInstance;
        }
    }

    public static class MapDelegate extends DefaultPersistenceDelegate {

        @Override
        protected Expression instantiate(Object oldInstance, Encoder out) {
            Map oldMap = (Map) oldInstance;
            HashMap newMap = new HashMap(oldMap);
            return new Expression(newMap, HashMap.class, "new", new Object[] {});
        }

        @Override
        protected boolean mutatesTo(Object oldInstance, Object newInstance) {
            return false;
        }

        @Override
        protected void initialize(Class<?> type, Object oldInstance, Object newInstance, Encoder out) {
            java.util.Collection oldO = (java.util.Collection) oldInstance;
            java.util.Collection newO = (java.util.Collection) newInstance;
            if (newO.size() != 0) {
                out.writeStatement(new Statement(oldInstance, "clear", new Object[] {}));
            }
            for (Iterator i = oldO.iterator(); i.hasNext(); ) {
                out.writeStatement(new Statement(oldInstance, "add", new Object[] { i.next() }));
            }
        }
    }

    public static class SetDelegate extends DefaultPersistenceDelegate {

        @Override
        protected Expression instantiate(Object oldInstance, Encoder out) {
            Set oldSet = (Set) oldInstance;
            HashSet newSet = new HashSet(oldSet);
            return new Expression(newSet, HashSet.class, "new", new Object[] {});
        }

        @Override
        protected boolean mutatesTo(Object oldInstance, Object newInstance) {
            return false;
        }

        @Override
        protected void initialize(Class<?> type, Object oldInstance, Object newInstance, Encoder out) {
            java.util.Collection oldO = (java.util.Collection) oldInstance;
            java.util.Collection newO = (java.util.Collection) newInstance;
            if (newO.size() != 0) {
                out.writeStatement(new Statement(oldInstance, "clear", new Object[] {}));
            }
            for (Iterator i = oldO.iterator(); i.hasNext(); ) {
                out.writeStatement(new Statement(oldInstance, "add", new Object[] { i.next() }));
            }
        }
    }

    public static class ListDelegate extends DefaultPersistenceDelegate {

        @Override
        protected Expression instantiate(Object oldInstance, Encoder out) {
            List oldList = (List) oldInstance;
            ArrayList newList = new ArrayList(oldList);
            return new Expression(newList, ArrayList.class, "new", new Object[] {});
        }

        @Override
        protected boolean mutatesTo(Object oldInstance, Object newInstance) {
            return false;
        }

        @Override
        protected void initialize(Class<?> type, Object oldInstance, Object newInstance, Encoder out) {
            java.util.Collection oldO = (java.util.Collection) oldInstance;
            java.util.Collection newO = (java.util.Collection) newInstance;
            if (newO.size() != 0) {
                out.writeStatement(new Statement(oldInstance, "clear", new Object[] {}));
            }
            for (Iterator i = oldO.iterator(); i.hasNext(); ) {
                out.writeStatement(new Statement(oldInstance, "add", new Object[] { i.next() }));
            }
        }
    }

    public static class DatePersistenceDelegate extends PersistenceDelegate {

        @Override
        protected Expression instantiate(Object oldInstance, Encoder out) {
            Date dateVal = (Date) oldInstance;
            Object[] args = { dateVal.getTime() };
            return new Expression(dateVal, dateVal.getClass(), "new", args);
        }

        @Override
        protected boolean mutatesTo(Object oldInstance, Object newInstance) {
            if (oldInstance == null || newInstance == null) {
                return false;
            }
            return oldInstance.getClass() == newInstance.getClass();
        }
    }

    public static class TimestampPersistenceDelegate extends DatePersistenceDelegate {

        @Override
        protected void initialize(Class<?> type, Object oldInstance, Object newInstance, Encoder out) {
            Timestamp ts = (Timestamp) oldInstance;
            Object[] args = { ts.getNanos() };
            Statement stmt = new Statement(oldInstance, "setNanos", args);
            out.writeStatement(stmt);
        }
    }

    public static void setMapRedWork(Configuration conf, MapredWork w, String hiveScratchDir) {
        setMapWork(conf, w.getMapWork(), hiveScratchDir, true);
        if (w.getReduceWork() != null) {
            setReduceWork(conf, w.getReduceWork(), hiveScratchDir, true);
        }
    }

    public static Path setMapWork(Configuration conf, MapWork w, String hiveScratchDir, boolean useCache) {
        return setBaseWork(conf, w, hiveScratchDir, MAP_PLAN_NAME, useCache);
    }

    public static Path setReduceWork(Configuration conf, ReduceWork w, String hiveScratchDir, boolean useCache) {
        return setBaseWork(conf, w, hiveScratchDir, REDUCE_PLAN_NAME, useCache);
    }

    private static Path setBaseWork(Configuration conf, BaseWork w, String hiveScratchDir, String name, boolean useCache) {
        try {
            setPlanPath(conf, hiveScratchDir);
            Path planPath = getPlanPath(conf, name);
            FileSystem fs = planPath.getFileSystem(conf);
            FSDataOutputStream out = fs.create(planPath);
            serializePlan(w, out, conf);
            if (useCache && !ShimLoader.getHadoopShims().isLocalMode(conf)) {
                if (!DistributedCache.getSymlink(conf)) {
                    DistributedCache.createSymlink(conf);
                }
                String uriWithLink = planPath.toUri().toString() + "#" + name;
                DistributedCache.addCacheFile(new URI(uriWithLink), conf);
                short replication = (short) conf.getInt("mapred.submit.replication", 10);
                fs.setReplication(planPath, replication);
            }
            gWorkMap.put(planPath, w);
            return planPath;
        } catch (Exception e) {
            e.printStackTrace();
            throw new RuntimeException(e);
        }
    }

    private static Path getPlanPath(Configuration conf, String name) {
        Path planPath = getPlanPath(conf);
        if (planPath == null) {
            return null;
        }
        return new Path(planPath, name);
    }

    private static void setPlanPath(Configuration conf, String hiveScratchDir) throws IOException {
        if (getPlanPath(conf) == null) {
            String jobID = UUID.randomUUID().toString();
            Path planPath = new Path(hiveScratchDir, jobID);
            FileSystem fs = planPath.getFileSystem(conf);
            fs.mkdirs(planPath);
            HiveConf.setVar(conf, HiveConf.ConfVars.PLAN, planPath.toUri().toString());
        }
    }

    public static Path getPlanPath(Configuration conf) {
        String plan = HiveConf.getVar(conf, HiveConf.ConfVars.PLAN);
        if (plan != null && !plan.isEmpty()) {
            return new Path(plan);
        }
        return null;
    }

    public static byte[] serializeExpressionToKryo(ExprNodeDesc expr) {
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        Output output = new Output(baos);
        runtimeSerializationKryo.get().writeClassAndObject(output, expr);
        output.close();
        return baos.toByteArray();
    }

    public static ExprNodeDesc deserializeExpressionFromKryo(byte[] bytes) {
        Input inp = new Input(new ByteArrayInputStream(bytes));
        Object o = runtimeSerializationKryo.get().readClassAndObject(inp);
        inp.close();
        return (o instanceof ExprNodeDesc) ? (ExprNodeDesc) o : null;
    }

    public static String serializeExpression(ExprNodeDesc expr) {
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        XMLEncoder encoder = new XMLEncoder(baos);
        encoder.setPersistenceDelegate(java.sql.Date.class, new DatePersistenceDelegate());
        encoder.setPersistenceDelegate(Timestamp.class, new TimestampPersistenceDelegate());
        try {
            encoder.writeObject(expr);
        } finally {
            encoder.close();
        }
        try {
            return baos.toString("UTF-8");
        } catch (UnsupportedEncodingException ex) {
            throw new RuntimeException("UTF-8 support required", ex);
        }
    }

    public static ExprNodeDesc deserializeExpression(String s, Configuration conf) {
        byte[] bytes;
        try {
            bytes = s.getBytes("UTF-8");
        } catch (UnsupportedEncodingException ex) {
            throw new RuntimeException("UTF-8 support required", ex);
        }
        ByteArrayInputStream bais = new ByteArrayInputStream(bytes);
        XMLDecoder decoder = new XMLDecoder(bais, null, null);
        try {
            ExprNodeDesc expr = (ExprNodeDesc) decoder.readObject();
            return expr;
        } finally {
            decoder.close();
        }
    }

    public static class CollectionPersistenceDelegate extends DefaultPersistenceDelegate {

        @Override
        protected Expression instantiate(Object oldInstance, Encoder out) {
            return new Expression(oldInstance, oldInstance.getClass(), "new", null);
        }

        @Override
        protected void initialize(Class type, Object oldInstance, Object newInstance, Encoder out) {
            Iterator ite = ((Collection) oldInstance).iterator();
            while (ite.hasNext()) {
                out.writeStatement(new Statement(oldInstance, "add", new Object[] { ite.next() }));
            }
        }
    }

    private static class SqlDateSerializer extends com.esotericsoftware.kryo.Serializer<java.sql.Date> {

        @Override
        public java.sql.Date read(Kryo kryo, Input input, Class<java.sql.Date> clazz) {
            return new java.sql.Date(input.readLong());
        }

        @Override
        public void write(Kryo kryo, Output output, java.sql.Date sqlDate) {
            output.writeLong(sqlDate.getTime());
        }
    }

    private static class CommonTokenSerializer extends com.esotericsoftware.kryo.Serializer<CommonToken> {

        @Override
        public CommonToken read(Kryo kryo, Input input, Class<CommonToken> clazz) {
            return new CommonToken(input.readInt(), input.readString());
        }

        @Override
        public void write(Kryo kryo, Output output, CommonToken token) {
            output.writeInt(token.getType());
            output.writeString(token.getText());
        }
    }

    private static void serializePlan(Object plan, OutputStream out, Configuration conf, boolean cloningPlan) {
        PerfLogger perfLogger = PerfLogger.getPerfLogger();
        perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.SERIALIZE_PLAN);
        String serializationType = conf.get(HiveConf.ConfVars.PLAN_SERIALIZATION.varname, "kryo");
        LOG.info("Serializing " + plan.getClass().getSimpleName() + " via " + serializationType);
        if ("javaXML".equalsIgnoreCase(serializationType)) {
            serializeObjectByJavaXML(plan, out);
        } else {
            if (cloningPlan) {
                serializeObjectByKryo(cloningQueryPlanKryo.get(), plan, out);
            } else {
                serializeObjectByKryo(runtimeSerializationKryo.get(), plan, out);
            }
        }
        perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.SERIALIZE_PLAN);
    }

    public static void serializePlan(Object plan, OutputStream out, Configuration conf) {
        serializePlan(plan, out, conf, false);
    }

    private static <T> T deserializePlan(InputStream in, Class<T> planClass, Configuration conf, boolean cloningPlan) {
        PerfLogger perfLogger = PerfLogger.getPerfLogger();
        perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.DESERIALIZE_PLAN);
        T plan;
        String serializationType = conf.get(HiveConf.ConfVars.PLAN_SERIALIZATION.varname, "kryo");
        LOG.info("Deserializing " + planClass.getSimpleName() + " via " + serializationType);
        if ("javaXML".equalsIgnoreCase(serializationType)) {
            plan = deserializeObjectByJavaXML(in);
        } else {
            if (cloningPlan) {
                plan = deserializeObjectByKryo(cloningQueryPlanKryo.get(), in, planClass);
            } else {
                plan = deserializeObjectByKryo(runtimeSerializationKryo.get(), in, planClass);
            }
        }
        perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.DESERIALIZE_PLAN);
        return plan;
    }

    public static <T> T deserializePlan(InputStream in, Class<T> planClass, Configuration conf) {
        return deserializePlan(in, planClass, conf, false);
    }

    public static MapredWork clonePlan(MapredWork plan) {
        PerfLogger perfLogger = PerfLogger.getPerfLogger();
        perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.CLONE_PLAN);
        ByteArrayOutputStream baos = new ByteArrayOutputStream(4096);
        Configuration conf = new Configuration();
        serializePlan(plan, baos, conf, true);
        MapredWork newPlan = deserializePlan(new ByteArrayInputStream(baos.toByteArray()), MapredWork.class, conf, true);
        perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.CLONE_PLAN);
        return newPlan;
    }

    private static void serializeObjectByJavaXML(Object plan, OutputStream out) {
        XMLEncoder e = new XMLEncoder(out);
        e.setExceptionListener(new ExceptionListener() {

            public void exceptionThrown(Exception e) {
                LOG.warn(org.apache.hadoop.util.StringUtils.stringifyException(e));
                throw new RuntimeException("Cannot serialize object", e);
            }
        });
        e.setPersistenceDelegate(ExpressionTypes.class, new EnumDelegate());
        e.setPersistenceDelegate(GroupByDesc.Mode.class, new EnumDelegate());
        e.setPersistenceDelegate(Operator.ProgressCounter.class, new EnumDelegate());
        e.setPersistenceDelegate(java.sql.Date.class, new DatePersistenceDelegate());
        e.setPersistenceDelegate(Timestamp.class, new TimestampPersistenceDelegate());
        e.setPersistenceDelegate(org.datanucleus.store.types.backed.Map.class, new MapDelegate());
        e.setPersistenceDelegate(org.datanucleus.store.types.backed.List.class, new ListDelegate());
        e.writeObject(plan);
        e.close();
    }

    private static void serializeObjectByKryo(Kryo kryo, Object plan, OutputStream out) {
        Output output = new Output(out);
        kryo.writeObject(output, plan);
        output.close();
    }

    @SuppressWarnings("unchecked")
    private static <T> T deserializeObjectByJavaXML(InputStream in) {
        XMLDecoder d = null;
        try {
            d = new XMLDecoder(in, null, null);
            return (T) d.readObject();
        } finally {
            if (null != d) {
                d.close();
            }
        }
    }

    private static <T> T deserializeObjectByKryo(Kryo kryo, InputStream in, Class<T> clazz) {
        Input inp = new Input(in);
        T t = kryo.readObject(inp, clazz);
        inp.close();
        return t;
    }

    private static ThreadLocal<Kryo> runtimeSerializationKryo = new ThreadLocal<Kryo>() {

        @Override
        protected synchronized Kryo initialValue() {
            Kryo kryo = new Kryo();
            kryo.setClassLoader(Thread.currentThread().getContextClassLoader());
            kryo.register(java.sql.Date.class, new SqlDateSerializer());
            removeField(kryo, Operator.class, "colExprMap");
            removeField(kryo, ColumnInfo.class, "objectInspector");
            removeField(kryo, MapWork.class, "opParseCtxMap");
            removeField(kryo, MapWork.class, "joinTree");
            return kryo;
        }
    };

    @SuppressWarnings("rawtypes")
    protected static void removeField(Kryo kryo, Class type, String fieldName) {
        FieldSerializer fld = new FieldSerializer(kryo, type);
        fld.removeField(fieldName);
        kryo.register(type, fld);
    }

    private static ThreadLocal<Kryo> cloningQueryPlanKryo = new ThreadLocal<Kryo>() {

        @Override
        protected synchronized Kryo initialValue() {
            Kryo kryo = new Kryo();
            kryo.setClassLoader(Thread.currentThread().getContextClassLoader());
            kryo.register(CommonToken.class, new CommonTokenSerializer());
            kryo.register(java.sql.Date.class, new SqlDateSerializer());
            return kryo;
        }
    };

    public static TableDesc defaultTd;

    static {
        defaultTd = PlanUtils.getDefaultTableDesc("" + Utilities.ctrlaCode);
    }

    public static final int carriageReturnCode = 13;

    public static final int newLineCode = 10;

    public static final int tabCode = 9;

    public static final int ctrlaCode = 1;

    public static final String INDENT = "  ";

    public static String nullStringStorage = "\\N";

    public static String nullStringOutput = "NULL";

    public static Random randGen = new Random();

    public static String getTaskId(Configuration hconf) {
        String taskid = (hconf == null) ? null : hconf.get("mapred.task.id");
        if ((taskid == null) || taskid.equals("")) {
            return ("" + Math.abs(randGen.nextInt()));
        } else {
            String ret = taskid.replaceAll(".*_[mr]_", "").replaceAll(".*_(map|reduce)_", "");
            return (ret);
        }
    }

    public static HashMap makeMap(Object... olist) {
        HashMap ret = new HashMap();
        for (int i = 0; i < olist.length; i += 2) {
            ret.put(olist[i], olist[i + 1]);
        }
        return (ret);
    }

    public static Properties makeProperties(String... olist) {
        Properties ret = new Properties();
        for (int i = 0; i < olist.length; i += 2) {
            ret.setProperty(olist[i], olist[i + 1]);
        }
        return (ret);
    }

    public static ArrayList makeList(Object... olist) {
        ArrayList ret = new ArrayList();
        for (Object element : olist) {
            ret.add(element);
        }
        return (ret);
    }

    public static class StreamPrinter extends Thread {

        InputStream is;

        String type;

        PrintStream os;

        public StreamPrinter(InputStream is, String type, PrintStream os) {
            this.is = is;
            this.type = type;
            this.os = os;
        }

        @Override
        public void run() {
            BufferedReader br = null;
            try {
                InputStreamReader isr = new InputStreamReader(is);
                br = new BufferedReader(isr);
                String line = null;
                if (type != null) {
                    while ((line = br.readLine()) != null) {
                        os.println(type + ">" + line);
                    }
                } else {
                    while ((line = br.readLine()) != null) {
                        os.println(line);
                    }
                }
                br.close();
                br = null;
            } catch (IOException ioe) {
                ioe.printStackTrace();
            } finally {
                IOUtils.closeStream(br);
            }
        }
    }

    public static TableDesc getTableDesc(Table tbl) {
        Properties props = tbl.getMetadata();
        props.put(serdeConstants.SERIALIZATION_LIB, tbl.getDeserializer().getClass().getName());
        return (new TableDesc(tbl.getInputFormatClass(), tbl.getOutputFormatClass(), props));
    }

    public static TableDesc getTableDesc(String cols, String colTypes) {
        return (new TableDesc(SequenceFileInputFormat.class, HiveSequenceFileOutputFormat.class, Utilities.makeProperties(serdeConstants.SERIALIZATION_FORMAT, "" + Utilities.ctrlaCode, serdeConstants.LIST_COLUMNS, cols, serdeConstants.LIST_COLUMN_TYPES, colTypes, serdeConstants.SERIALIZATION_LIB, LazySimpleSerDe.class.getName())));
    }

    public static PartitionDesc getPartitionDesc(Partition part) throws HiveException {
        return (new PartitionDesc(part));
    }

    public static PartitionDesc getPartitionDescFromTableDesc(TableDesc tblDesc, Partition part) throws HiveException {
        return new PartitionDesc(part, tblDesc);
    }

    private static String getOpTreeSkel_helper(Operator<?> op, String indent) {
        if (op == null) {
            return "";
        }
        StringBuilder sb = new StringBuilder();
        sb.append(indent);
        sb.append(op.toString());
        sb.append("\n");
        if (op.getChildOperators() != null) {
            for (Object child : op.getChildOperators()) {
                sb.append(getOpTreeSkel_helper((Operator<?>) child, indent + "  "));
            }
        }
        return sb.toString();
    }

    public static String getOpTreeSkel(Operator<?> op) {
        return getOpTreeSkel_helper(op, "");
    }

    private static boolean isWhitespace(int c) {
        if (c == -1) {
            return false;
        }
        return Character.isWhitespace((char) c);
    }

    public static boolean contentsEqual(InputStream is1, InputStream is2, boolean ignoreWhitespace) throws IOException {
        try {
            if ((is1 == is2) || (is1 == null && is2 == null)) {
                return true;
            }
            if (is1 == null || is2 == null) {
                return false;
            }
            while (true) {
                int c1 = is1.read();
                while (ignoreWhitespace && isWhitespace(c1)) {
                    c1 = is1.read();
                }
                int c2 = is2.read();
                while (ignoreWhitespace && isWhitespace(c2)) {
                    c2 = is2.read();
                }
                if (c1 == -1 && c2 == -1) {
                    return true;
                }
                if (c1 != c2) {
                    break;
                }
            }
        } catch (FileNotFoundException e) {
            e.printStackTrace();
        }
        return false;
    }

    public static String abbreviate(String str, int max) {
        str = str.trim();
        int len = str.length();
        int suffixlength = 20;
        if (len <= max) {
            return str;
        }
        suffixlength = Math.min(suffixlength, (max - 3) / 2);
        String rev = StringUtils.reverse(str);
        String suffix = WordUtils.abbreviate(rev, 0, suffixlength, "");
        suffix = StringUtils.reverse(suffix);
        String prefix = StringUtils.abbreviate(str, max - suffix.length());
        return prefix + suffix;
    }

    public static final String NSTR = "";

    public static enum StreamStatus {

        EOF, TERMINATED
    }

    public static StreamStatus readColumn(DataInput in, OutputStream out) throws IOException {
        boolean foundCrChar = false;
        while (true) {
            int b;
            try {
                b = in.readByte();
            } catch (EOFException e) {
                return StreamStatus.EOF;
            }
            if (Shell.WINDOWS) {
                if (foundCrChar && b != Utilities.newLineCode) {
                    out.write(Utilities.carriageReturnCode);
                    foundCrChar = false;
                }
                if (b == Utilities.carriageReturnCode) {
                    foundCrChar = true;
                    continue;
                }
            }
            if (b == Utilities.newLineCode) {
                return StreamStatus.TERMINATED;
            }
            out.write(b);
        }
    }

    public static OutputStream createCompressedStream(JobConf jc, OutputStream out) throws IOException {
        boolean isCompressed = FileOutputFormat.getCompressOutput(jc);
        return createCompressedStream(jc, out, isCompressed);
    }

    public static OutputStream createCompressedStream(JobConf jc, OutputStream out, boolean isCompressed) throws IOException {
        if (isCompressed) {
            Class<? extends CompressionCodec> codecClass = FileOutputFormat.getOutputCompressorClass(jc, DefaultCodec.class);
            CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, jc);
            return codec.createOutputStream(out);
        } else {
            return (out);
        }
    }

    @Deprecated
    public static String getFileExtension(JobConf jc, boolean isCompressed) {
        return getFileExtension(jc, isCompressed, new HiveIgnoreKeyTextOutputFormat());
    }

    public static String getFileExtension(JobConf jc, boolean isCompressed, HiveOutputFormat<?, ?> hiveOutputFormat) {
        String extension = HiveConf.getVar(jc, HiveConf.ConfVars.OUTPUT_FILE_EXTENSION);
        if (!StringUtils.isEmpty(extension)) {
            return extension;
        }
        if ((hiveOutputFormat instanceof HiveIgnoreKeyTextOutputFormat) && isCompressed) {
            Class<? extends CompressionCodec> codecClass = FileOutputFormat.getOutputCompressorClass(jc, DefaultCodec.class);
            CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, jc);
            return codec.getDefaultExtension();
        }
        return "";
    }

    public static SequenceFile.Writer createSequenceWriter(JobConf jc, FileSystem fs, Path file, Class<?> keyClass, Class<?> valClass) throws IOException {
        boolean isCompressed = FileOutputFormat.getCompressOutput(jc);
        return createSequenceWriter(jc, fs, file, keyClass, valClass, isCompressed);
    }

    public static SequenceFile.Writer createSequenceWriter(JobConf jc, FileSystem fs, Path file, Class<?> keyClass, Class<?> valClass, boolean isCompressed) throws IOException {
        CompressionCodec codec = null;
        CompressionType compressionType = CompressionType.NONE;
        Class codecClass = null;
        if (isCompressed) {
            compressionType = SequenceFileOutputFormat.getOutputCompressionType(jc);
            codecClass = FileOutputFormat.getOutputCompressorClass(jc, DefaultCodec.class);
            codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, jc);
        }
        return (SequenceFile.createWriter(fs, jc, file, keyClass, valClass, compressionType, codec));
    }

    public static RCFile.Writer createRCFileWriter(JobConf jc, FileSystem fs, Path file, boolean isCompressed) throws IOException {
        CompressionCodec codec = null;
        Class<?> codecClass = null;
        if (isCompressed) {
            codecClass = FileOutputFormat.getOutputCompressorClass(jc, DefaultCodec.class);
            codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, jc);
        }
        return new RCFile.Writer(fs, jc, file, null, codec);
    }

    public static String realFile(String newFile, Configuration conf) throws IOException {
        Path path = new Path(newFile);
        URI pathURI = path.toUri();
        FileSystem fs;
        if (pathURI.getScheme() == null) {
            fs = FileSystem.getLocal(conf);
        } else {
            fs = path.getFileSystem(conf);
        }
        if (!fs.exists(path)) {
            return null;
        }
        String file = path.makeQualified(fs).toString();
        if (StringUtils.startsWith(file, "file:/") && !StringUtils.startsWith(file, "file:///")) {
            file = "file:///" + file.substring("file:/".length());
        }
        return file;
    }

    public static List<String> mergeUniqElems(List<String> src, List<String> dest) {
        if (dest == null) {
            return src;
        }
        if (src == null) {
            return dest;
        }
        int pos = 0;
        while (pos < dest.size()) {
            if (!src.contains(dest.get(pos))) {
                src.add(dest.get(pos));
            }
            pos++;
        }
        return src;
    }

    private static final String tmpPrefix = "_tmp.";

    private static final String taskTmpPrefix = "_task_tmp.";

    public static Path toTaskTempPath(Path orig) {
        if (orig.getName().indexOf(taskTmpPrefix) == 0) {
            return orig;
        }
        return new Path(orig.getParent(), taskTmpPrefix + orig.getName());
    }

    public static Path toTaskTempPath(String orig) {
        return toTaskTempPath(new Path(orig));
    }

    public static Path toTempPath(Path orig) {
        if (orig.getName().indexOf(tmpPrefix) == 0) {
            return orig;
        }
        return new Path(orig.getParent(), tmpPrefix + orig.getName());
    }

    public static Path toTempPath(String orig) {
        return toTempPath(new Path(orig));
    }

    public static boolean isTempPath(FileStatus file) {
        String name = file.getPath().getName();
        return (name.startsWith("_task") || name.startsWith(tmpPrefix));
    }

    public static void rename(FileSystem fs, Path src, Path dst) throws IOException, HiveException {
        if (!fs.rename(src, dst)) {
            throw new HiveException("Unable to move: " + src + " to: " + dst);
        }
    }

    public static void renameOrMoveFiles(FileSystem fs, Path src, Path dst) throws IOException, HiveException {
        if (!fs.exists(dst)) {
            if (!fs.rename(src, dst)) {
                throw new HiveException("Unable to move: " + src + " to: " + dst);
            }
        } else {
            FileStatus[] files = fs.listStatus(src);
            for (FileStatus file : files) {
                Path srcFilePath = file.getPath();
                String fileName = srcFilePath.getName();
                Path dstFilePath = new Path(dst, fileName);
                if (file.isDir()) {
                    renameOrMoveFiles(fs, srcFilePath, dstFilePath);
                } else {
                    if (fs.exists(dstFilePath)) {
                        int suffix = 0;
                        do {
                            suffix++;
                            dstFilePath = new Path(dst, fileName + "_" + suffix);
                        } while (fs.exists(dstFilePath));
                    }
                    if (!fs.rename(srcFilePath, dstFilePath)) {
                        throw new HiveException("Unable to move: " + src + " to: " + dst);
                    }
                }
            }
        }
    }

    private static final Pattern FILE_NAME_TO_TASK_ID_REGEX = Pattern.compile("^.*?([0-9]+)(_[0-9]{1,3})?(\\..*)?$");

    private static final Pattern FILE_NAME_PREFIXED_TASK_ID_REGEX = Pattern.compile("^.*?((\\(.*\\))?[0-9]+)(_[0-9]{1,3})?(\\..*)?$");

    private static final Pattern PREFIXED_TASK_ID_REGEX = Pattern.compile("^(.*?\\(.*\\))?([0-9]+)$");

    public static String getTaskIdFromFilename(String filename) {
        return getIdFromFilename(filename, FILE_NAME_TO_TASK_ID_REGEX);
    }

    public static String getPrefixedTaskIdFromFilename(String filename) {
        return getIdFromFilename(filename, FILE_NAME_PREFIXED_TASK_ID_REGEX);
    }

    private static String getIdFromFilename(String filename, Pattern pattern) {
        String taskId = filename;
        int dirEnd = filename.lastIndexOf(Path.SEPARATOR);
        if (dirEnd != -1) {
            taskId = filename.substring(dirEnd + 1);
        }
        Matcher m = pattern.matcher(taskId);
        if (!m.matches()) {
            LOG.warn("Unable to get task id from file name: " + filename + ". Using last component" + taskId + " as task id.");
        } else {
            taskId = m.group(1);
        }
        LOG.debug("TaskId for " + filename + " = " + taskId);
        return taskId;
    }

    public static String getFileNameFromDirName(String dirName) {
        int dirEnd = dirName.lastIndexOf(Path.SEPARATOR);
        if (dirEnd != -1) {
            return dirName.substring(dirEnd + 1);
        }
        return dirName;
    }

    public static String replaceTaskIdFromFilename(String filename, int bucketNum) {
        return replaceTaskIdFromFilename(filename, String.valueOf(bucketNum));
    }

    public static String replaceTaskIdFromFilename(String filename, String fileId) {
        String taskId = getTaskIdFromFilename(filename);
        String newTaskId = replaceTaskId(taskId, fileId);
        String ret = replaceTaskIdFromFilename(filename, taskId, newTaskId);
        return (ret);
    }

    private static String replaceTaskId(String taskId, int bucketNum) {
        return replaceTaskId(taskId, String.valueOf(bucketNum));
    }

    private static String replaceTaskId(String taskId, String strBucketNum) {
        Matcher m = PREFIXED_TASK_ID_REGEX.matcher(strBucketNum);
        if (!m.matches()) {
            LOG.warn("Unable to determine bucket number from file ID: " + strBucketNum + ". Using " + "file ID as bucket number.");
            return adjustBucketNumLen(strBucketNum, taskId);
        } else {
            String adjustedBucketNum = adjustBucketNumLen(m.group(2), taskId);
            return (m.group(1) == null ? "" : m.group(1)) + adjustedBucketNum;
        }
    }

    private static String adjustBucketNumLen(String bucketNum, String taskId) {
        int bucketNumLen = bucketNum.length();
        int taskIdLen = taskId.length();
        StringBuffer s = new StringBuffer();
        for (int i = 0; i < taskIdLen - bucketNumLen; i++) {
            s.append("0");
        }
        return s.toString() + bucketNum;
    }

    private static String replaceTaskIdFromFilename(String filename, String oldTaskId, String newTaskId) {
        String[] spl = filename.split(oldTaskId);
        if ((spl.length == 0) || (spl.length == 1)) {
            return filename.replaceAll(oldTaskId, newTaskId);
        }
        StringBuffer snew = new StringBuffer();
        for (int idx = 0; idx < spl.length - 1; idx++) {
            if (idx > 0) {
                snew.append(oldTaskId);
            }
            snew.append(spl[idx]);
        }
        snew.append(newTaskId);
        snew.append(spl[spl.length - 1]);
        return snew.toString();
    }

    public static FileStatus[] listStatusIfExists(Path path, FileSystem fs) throws IOException {
        try {
            return fs.listStatus(path);
        } catch (FileNotFoundException e) {
            return null;
        }
    }

    public static FileStatus[] getFileStatusRecurse(Path path, int level, FileSystem fs) throws IOException {
        StringBuilder sb = new StringBuilder(path.toUri().getPath());
        for (int i = 0; i < level; ++i) {
            sb.append(Path.SEPARATOR).append("*");
        }
        Path pathPattern = new Path(path, sb.toString());
        return fs.globStatus(pathPattern);
    }

    public static void mvFileToFinalPath(String specPath, Configuration hconf, boolean success, Log log, DynamicPartitionCtx dpCtx, FileSinkDesc conf, Reporter reporter) throws IOException, HiveException {
        FileSystem fs = (new Path(specPath)).getFileSystem(hconf);
        Path tmpPath = Utilities.toTempPath(specPath);
        Path taskTmpPath = Utilities.toTaskTempPath(specPath);
        Path intermediatePath = new Path(tmpPath.getParent(), tmpPath.getName() + ".intermediate");
        Path finalPath = new Path(specPath);
        if (success) {
            if (fs.exists(tmpPath)) {
                log.info("Moving tmp dir: " + tmpPath + " to: " + intermediatePath);
                Utilities.rename(fs, tmpPath, intermediatePath);
                ArrayList<String> emptyBuckets = Utilities.removeTempOrDuplicateFiles(fs, intermediatePath, dpCtx);
                if (emptyBuckets.size() > 0) {
                    createEmptyBuckets(hconf, emptyBuckets, conf, reporter);
                }
                log.info("Moving tmp dir: " + intermediatePath + " to: " + finalPath);
                Utilities.renameOrMoveFiles(fs, intermediatePath, finalPath);
            }
        } else {
            fs.delete(tmpPath, true);
        }
        fs.delete(taskTmpPath, true);
    }

    private static void createEmptyBuckets(Configuration hconf, ArrayList<String> paths, FileSinkDesc conf, Reporter reporter) throws HiveException, IOException {
        JobConf jc;
        if (hconf instanceof JobConf) {
            jc = new JobConf(hconf);
        } else {
            jc = new JobConf(hconf);
        }
        HiveOutputFormat<?, ?> hiveOutputFormat = null;
        Class<? extends Writable> outputClass = null;
        boolean isCompressed = conf.getCompressed();
        TableDesc tableInfo = conf.getTableInfo();
        try {
            Serializer serializer = (Serializer) tableInfo.getDeserializerClass().newInstance();
            serializer.initialize(null, tableInfo.getProperties());
            outputClass = serializer.getSerializedClass();
            hiveOutputFormat = conf.getTableInfo().getOutputFileFormatClass().newInstance();
        } catch (SerDeException e) {
            throw new HiveException(e);
        } catch (InstantiationException e) {
            throw new HiveException(e);
        } catch (IllegalAccessException e) {
            throw new HiveException(e);
        }
        for (String p : paths) {
            Path path = new Path(p);
            RecordWriter writer = HiveFileFormatUtils.getRecordWriter(jc, hiveOutputFormat, outputClass, isCompressed, tableInfo.getProperties(), path, reporter);
            writer.close(false);
            LOG.info("created empty bucket for enforcing bucketing at " + path);
        }
    }

    public static void removeTempOrDuplicateFiles(FileSystem fs, Path path) throws IOException {
        removeTempOrDuplicateFiles(fs, path, null);
    }

    public static ArrayList<String> removeTempOrDuplicateFiles(FileSystem fs, Path path, DynamicPartitionCtx dpCtx) throws IOException {
        if (path == null) {
            return null;
        }
        ArrayList<String> result = new ArrayList<String>();
        if (dpCtx != null) {
            FileStatus[] parts = getFileStatusRecurse(path, dpCtx.getNumDPCols(), fs);
            HashMap<String, FileStatus> taskIDToFile = null;
            for (int i = 0; i < parts.length; ++i) {
                assert parts[i].isDir() : "dynamic partition " + parts[i].getPath() + " is not a direcgtory";
                FileStatus[] items = fs.listStatus(parts[i].getPath());
                if (items.length == 0) {
                    if (!fs.delete(parts[i].getPath(), true)) {
                        LOG.error("Cannot delete empty directory " + parts[i].getPath());
                        throw new IOException("Cannot delete empty directory " + parts[i].getPath());
                    }
                }
                taskIDToFile = removeTempOrDuplicateFiles(items, fs);
                if (dpCtx.getNumBuckets() > 0 && taskIDToFile != null) {
                    items = fs.listStatus(parts[i].getPath());
                    String taskID1 = taskIDToFile.keySet().iterator().next();
                    Path bucketPath = taskIDToFile.values().iterator().next().getPath();
                    for (int j = 0; j < dpCtx.getNumBuckets(); ++j) {
                        String taskID2 = replaceTaskId(taskID1, j);
                        if (!taskIDToFile.containsKey(taskID2)) {
                            String path2 = replaceTaskIdFromFilename(bucketPath.toUri().getPath().toString(), j);
                            result.add(path2);
                        }
                    }
                }
            }
        } else {
            FileStatus[] items = fs.listStatus(path);
            removeTempOrDuplicateFiles(items, fs);
        }
        return result;
    }

    public static HashMap<String, FileStatus> removeTempOrDuplicateFiles(FileStatus[] items, FileSystem fs) throws IOException {
        if (items == null || fs == null) {
            return null;
        }
        HashMap<String, FileStatus> taskIdToFile = new HashMap<String, FileStatus>();
        for (FileStatus one : items) {
            if (isTempPath(one)) {
                if (!fs.delete(one.getPath(), true)) {
                    throw new IOException("Unable to delete tmp file: " + one.getPath());
                }
            } else {
                String taskId = getPrefixedTaskIdFromFilename(one.getPath().getName());
                FileStatus otherFile = taskIdToFile.get(taskId);
                if (otherFile == null) {
                    taskIdToFile.put(taskId, one);
                } else {
                    FileStatus toDelete = null;
                    if (otherFile.getLen() >= one.getLen()) {
                        toDelete = one;
                    } else {
                        toDelete = otherFile;
                        taskIdToFile.put(taskId, one);
                    }
                    long len1 = toDelete.getLen();
                    long len2 = taskIdToFile.get(taskId).getLen();
                    if (!fs.delete(toDelete.getPath(), true)) {
                        throw new IOException("Unable to delete duplicate file: " + toDelete.getPath() + ". Existing file: " + taskIdToFile.get(taskId).getPath());
                    } else {
                        LOG.warn("Duplicate taskid file removed: " + toDelete.getPath() + " with length " + len1 + ". Existing file: " + taskIdToFile.get(taskId).getPath() + " with length " + len2);
                    }
                }
            }
        }
        return taskIdToFile;
    }

    public static String getNameMessage(Exception e) {
        return e.getClass().getName() + "(" + e.getMessage() + ")";
    }

    public static String getResourceFiles(Configuration conf, SessionState.ResourceType t) {
        SessionState ss = SessionState.get();
        Set<String> files = (ss == null) ? null : ss.list_resource(t, null);
        if (files != null) {
            List<String> realFiles = new ArrayList<String>(files.size());
            for (String one : files) {
                try {
                    realFiles.add(realFile(one, conf));
                } catch (IOException e) {
                    throw new RuntimeException("Cannot validate file " + one + "due to exception: " + e.getMessage(), e);
                }
            }
            return StringUtils.join(realFiles, ",");
        } else {
            return "";
        }
    }

    public static ClassLoader addToClassPath(ClassLoader cloader, String[] newPaths) throws Exception {
        URLClassLoader loader = (URLClassLoader) cloader;
        List<URL> curPath = Arrays.asList(loader.getURLs());
        ArrayList<URL> newPath = new ArrayList<URL>();
        for (URL onePath : curPath) {
            newPath.add(onePath);
        }
        curPath = newPath;
        for (String onestr : newPaths) {
            if (StringUtils.indexOf(onestr, "file://") == 0) {
                onestr = StringUtils.substring(onestr, 7);
            }
            URL oneurl = (new File(onestr)).toURL();
            if (!curPath.contains(oneurl)) {
                curPath.add(oneurl);
            }
        }
        return new URLClassLoader(curPath.toArray(new URL[0]), loader);
    }

    public static void removeFromClassPath(String[] pathsToRemove) throws Exception {
        Thread curThread = Thread.currentThread();
        URLClassLoader loader = (URLClassLoader) curThread.getContextClassLoader();
        Set<URL> newPath = new HashSet<URL>(Arrays.asList(loader.getURLs()));
        for (String onestr : pathsToRemove) {
            if (StringUtils.indexOf(onestr, "file://") == 0) {
                onestr = StringUtils.substring(onestr, 7);
            }
            URL oneurl = (new File(onestr)).toURL();
            newPath.remove(oneurl);
        }
        loader = new URLClassLoader(newPath.toArray(new URL[0]));
        curThread.setContextClassLoader(loader);
        SessionState.get().getConf().setClassLoader(loader);
    }

    public static String formatBinaryString(byte[] array, int start, int length) {
        StringBuilder sb = new StringBuilder();
        for (int i = start; i < start + length; i++) {
            sb.append("x");
            sb.append(array[i] < 0 ? array[i] + 256 : array[i] + 0);
        }
        return sb.toString();
    }

    public static List<String> getColumnNamesFromSortCols(List<Order> sortCols) {
        List<String> names = new ArrayList<String>();
        for (Order o : sortCols) {
            names.add(o.getCol());
        }
        return names;
    }

    public static List<String> getColumnNamesFromFieldSchema(List<FieldSchema> partCols) {
        List<String> names = new ArrayList<String>();
        for (FieldSchema o : partCols) {
            names.add(o.getName());
        }
        return names;
    }

    public static List<String> getColumnNames(Properties props) {
        List<String> names = new ArrayList<String>();
        String colNames = props.getProperty(serdeConstants.LIST_COLUMNS);
        String[] cols = colNames.trim().split(",");
        if (cols != null) {
            for (String col : cols) {
                if (col != null && !col.trim().equals("")) {
                    names.add(col);
                }
            }
        }
        return names;
    }

    public static List<String> getColumnTypes(Properties props) {
        List<String> names = new ArrayList<String>();
        String colNames = props.getProperty(serdeConstants.LIST_COLUMN_TYPES);
        String[] cols = colNames.trim().split(",");
        if (cols != null) {
            for (String col : cols) {
                if (col != null && !col.trim().equals("")) {
                    names.add(col);
                }
            }
        }
        return names;
    }

    public static void validateColumnNames(List<String> colNames, List<String> checkCols) throws SemanticException {
        Iterator<String> checkColsIter = checkCols.iterator();
        while (checkColsIter.hasNext()) {
            String toCheck = checkColsIter.next();
            boolean found = false;
            Iterator<String> colNamesIter = colNames.iterator();
            while (colNamesIter.hasNext()) {
                String colName = colNamesIter.next();
                if (toCheck.equalsIgnoreCase(colName)) {
                    found = true;
                    break;
                }
            }
            if (!found) {
                throw new SemanticException(ErrorMsg.INVALID_COLUMN.getMsg());
            }
        }
    }

    public static int getDefaultNotificationInterval(Configuration hconf) {
        int notificationInterval;
        Integer expInterval = Integer.decode(hconf.get("mapred.tasktracker.expiry.interval"));
        if (expInterval != null) {
            notificationInterval = expInterval.intValue() / 2;
        } else {
            notificationInterval = 5 * 60 * 1000;
        }
        return notificationInterval;
    }

    public static void copyTableJobPropertiesToConf(TableDesc tbl, JobConf job) {
        Map<String, String> jobProperties = tbl.getJobProperties();
        if (jobProperties == null) {
            return;
        }
        for (Map.Entry<String, String> entry : jobProperties.entrySet()) {
            job.set(entry.getKey(), entry.getValue());
        }
    }

    public static Object INPUT_SUMMARY_LOCK = new Object();

    public static ContentSummary getInputSummary(Context ctx, MapWork work, PathFilter filter) throws IOException {
        PerfLogger perfLogger = PerfLogger.getPerfLogger();
        perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.INPUT_SUMMARY);
        long[] summary = { 0, 0, 0 };
        List<String> pathNeedProcess = new ArrayList<String>();
        synchronized (INPUT_SUMMARY_LOCK) {
            for (String path : work.getPathToAliases().keySet()) {
                Path p = new Path(path);
                if (filter != null && !filter.accept(p)) {
                    continue;
                }
                ContentSummary cs = ctx.getCS(path);
                if (cs == null) {
                    if (path == null) {
                        continue;
                    }
                    pathNeedProcess.add(path);
                } else {
                    summary[0] += cs.getLength();
                    summary[1] += cs.getFileCount();
                    summary[2] += cs.getDirectoryCount();
                }
            }
            final Map<String, ContentSummary> resultMap = new ConcurrentHashMap<String, ContentSummary>();
            ArrayList<Future<?>> results = new ArrayList<Future<?>>();
            final ThreadPoolExecutor executor;
            int maxThreads = ctx.getConf().getInt("mapred.dfsclient.parallelism.max", 0);
            if (pathNeedProcess.size() > 1 && maxThreads > 1) {
                int numExecutors = Math.min(pathNeedProcess.size(), maxThreads);
                LOG.info("Using " + numExecutors + " threads for getContentSummary");
                executor = new ThreadPoolExecutor(numExecutors, numExecutors, 60, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>());
            } else {
                executor = null;
            }
            HiveInterruptCallback interrup = HiveInterruptUtils.add(new HiveInterruptCallback() {

                @Override
                public void interrupt() {
                    if (executor != null) {
                        executor.shutdownNow();
                    }
                }
            });
            try {
                Configuration conf = ctx.getConf();
                JobConf jobConf = new JobConf(conf);
                for (String path : pathNeedProcess) {
                    final Path p = new Path(path);
                    final String pathStr = path;
                    final Configuration myConf = conf;
                    final JobConf myJobConf = jobConf;
                    final PartitionDesc partDesc = work.getPathToPartitionInfo().get(p.toString());
                    Runnable r = new Runnable() {

                        public void run() {
                            try {
                                ContentSummary resultCs;
                                Class<? extends InputFormat> inputFormatCls = partDesc.getInputFileFormatClass();
                                InputFormat inputFormatObj = HiveInputFormat.getInputFormatFromCache(inputFormatCls, myJobConf);
                                if (inputFormatObj instanceof ContentSummaryInputFormat) {
                                    resultCs = ((ContentSummaryInputFormat) inputFormatObj).getContentSummary(p, myJobConf);
                                } else {
                                    FileSystem fs = p.getFileSystem(myConf);
                                    resultCs = fs.getContentSummary(p);
                                }
                                resultMap.put(pathStr, resultCs);
                            } catch (IOException e) {
                                LOG.info("Cannot get size of " + pathStr + ". Safely ignored.");
                            }
                        }
                    };
                    if (executor == null) {
                        r.run();
                    } else {
                        Future<?> result = executor.submit(r);
                        results.add(result);
                    }
                }
                if (executor != null) {
                    for (Future<?> result : results) {
                        boolean executorDone = false;
                        do {
                            try {
                                result.get();
                                executorDone = true;
                            } catch (InterruptedException e) {
                                LOG.info("Interrupted when waiting threads: ", e);
                                Thread.currentThread().interrupt();
                                break;
                            } catch (ExecutionException e) {
                                throw new IOException(e);
                            }
                        } while (!executorDone);
                    }
                    executor.shutdown();
                }
                HiveInterruptUtils.checkInterrupted();
                for (Map.Entry<String, ContentSummary> entry : resultMap.entrySet()) {
                    ContentSummary cs = entry.getValue();
                    summary[0] += cs.getLength();
                    summary[1] += cs.getFileCount();
                    summary[2] += cs.getDirectoryCount();
                    ctx.addCS(entry.getKey(), cs);
                    LOG.info("Cache Content Summary for " + entry.getKey() + " length: " + cs.getLength() + " file count: " + cs.getFileCount() + " directory count: " + cs.getDirectoryCount());
                }
                perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.INPUT_SUMMARY);
                return new ContentSummary(summary[0], summary[1], summary[2]);
            } finally {
                HiveInterruptUtils.remove(interrup);
            }
        }
    }

    public static boolean isEmptyPath(JobConf job, Path dirPath, Context ctx) throws Exception {
        ContentSummary cs = ctx.getCS(dirPath);
        if (cs != null) {
            LOG.info("Content Summary " + dirPath + "length: " + cs.getLength() + " num files: " + cs.getFileCount() + " num directories: " + cs.getDirectoryCount());
            return (cs.getLength() == 0 && cs.getFileCount() == 0 && cs.getDirectoryCount() <= 1);
        } else {
            LOG.info("Content Summary not cached for " + dirPath);
        }
        return isEmptyPath(job, dirPath);
    }

    public static boolean isEmptyPath(JobConf job, Path dirPath) throws Exception {
        FileSystem inpFs = dirPath.getFileSystem(job);
        if (inpFs.exists(dirPath)) {
            FileStatus[] fStats = inpFs.listStatus(dirPath);
            if (fStats.length > 0) {
                return false;
            }
        }
        return true;
    }

    public static List<ExecDriver> getMRTasks(List<Task<? extends Serializable>> tasks) {
        List<ExecDriver> mrTasks = new ArrayList<ExecDriver>();
        if (tasks != null) {
            getMRTasks(tasks, mrTasks);
        }
        return mrTasks;
    }

    private static void getMRTasks(List<Task<? extends Serializable>> tasks, List<ExecDriver> mrTasks) {
        for (Task<? extends Serializable> task : tasks) {
            if (task instanceof ExecDriver && !mrTasks.contains((ExecDriver) task)) {
                mrTasks.add((ExecDriver) task);
            }
            if (task.getDependentTasks() != null) {
                getMRTasks(task.getDependentTasks(), mrTasks);
            }
        }
    }

    public static List<LinkedHashMap<String, String>> getFullDPSpecs(Configuration conf, DynamicPartitionCtx dpCtx) throws HiveException {
        try {
            Path loadPath = new Path(dpCtx.getRootPath());
            FileSystem fs = loadPath.getFileSystem(conf);
            int numDPCols = dpCtx.getNumDPCols();
            FileStatus[] status = Utilities.getFileStatusRecurse(loadPath, numDPCols, fs);
            if (status.length == 0) {
                LOG.warn("No partition is generated by dynamic partitioning");
                return null;
            }
            Map<String, String> partSpec = dpCtx.getPartSpec();
            List<LinkedHashMap<String, String>> fullPartSpecs = new ArrayList<LinkedHashMap<String, String>>();
            for (int i = 0; i < status.length; ++i) {
                Path partPath = status[i].getPath();
                assert fs.getFileStatus(partPath).isDir() : "partitions " + partPath + " is not a directory !";
                LinkedHashMap<String, String> fullPartSpec = new LinkedHashMap<String, String>(partSpec);
                Warehouse.makeSpecFromName(fullPartSpec, partPath);
                fullPartSpecs.add(fullPartSpec);
            }
            return fullPartSpecs;
        } catch (IOException e) {
            throw new HiveException(e);
        }
    }

    public static StatsPublisher getStatsPublisher(JobConf jc) {
        String statsImplementationClass = HiveConf.getVar(jc, HiveConf.ConfVars.HIVESTATSDBCLASS);
        if (StatsFactory.setImplementation(statsImplementationClass, jc)) {
            return StatsFactory.getStatsPublisher();
        } else {
            return null;
        }
    }

    public static String getHashedStatsPrefix(String statsPrefix, int maxPrefixLength) {
        String ret = statsPrefix;
        if (maxPrefixLength >= 0 && statsPrefix.length() > maxPrefixLength) {
            try {
                MessageDigest digester = MessageDigest.getInstance("MD5");
                digester.update(statsPrefix.getBytes());
                ret = new String(digester.digest()) + Path.SEPARATOR;
            } catch (NoSuchAlgorithmException e) {
                throw new RuntimeException(e);
            }
        }
        return ret;
    }

    public static void setColumnNameList(JobConf jobConf, Operator op) {
        RowSchema rowSchema = op.getSchema();
        if (rowSchema == null) {
            return;
        }
        StringBuilder columnNames = new StringBuilder();
        for (ColumnInfo colInfo : rowSchema.getSignature()) {
            if (columnNames.length() > 0) {
                columnNames.append(",");
            }
            columnNames.append(colInfo.getInternalName());
        }
        String columnNamesString = columnNames.toString();
        jobConf.set(serdeConstants.LIST_COLUMNS, columnNamesString);
    }

    public static void setColumnTypeList(JobConf jobConf, Operator op) {
        RowSchema rowSchema = op.getSchema();
        if (rowSchema == null) {
            return;
        }
        StringBuilder columnTypes = new StringBuilder();
        for (ColumnInfo colInfo : rowSchema.getSignature()) {
            if (columnTypes.length() > 0) {
                columnTypes.append(",");
            }
            columnTypes.append(colInfo.getTypeName());
        }
        String columnTypesString = columnTypes.toString();
        jobConf.set(serdeConstants.LIST_COLUMN_TYPES, columnTypesString);
    }

    public static void validatePartSpec(Table tbl, Map<String, String> partSpec) throws SemanticException {
        List<FieldSchema> parts = tbl.getPartitionKeys();
        Set<String> partCols = new HashSet<String>(parts.size());
        for (FieldSchema col : parts) {
            partCols.add(col.getName());
        }
        for (String col : partSpec.keySet()) {
            if (!partCols.contains(col)) {
                throw new SemanticException(ErrorMsg.NONEXISTPARTCOL.getMsg(col));
            }
        }
    }

    public static String suffix = ".hashtable";

    public static String generatePath(String baseURI, String dumpFilePrefix, Byte tag, String bigBucketFileName) {
        String path = new String(baseURI + Path.SEPARATOR + "MapJoin-" + dumpFilePrefix + tag + "-" + bigBucketFileName + suffix);
        return path;
    }

    public static String generateFileName(Byte tag, String bigBucketFileName) {
        String fileName = new String("MapJoin-" + tag + "-" + bigBucketFileName + suffix);
        return fileName;
    }

    public static String generateTmpURI(String baseURI, String id) {
        String tmpFileURI = new String(baseURI + Path.SEPARATOR + "HashTable-" + id);
        return tmpFileURI;
    }

    public static String generateTarURI(String baseURI, String filename) {
        String tmpFileURI = new String(baseURI + Path.SEPARATOR + filename + ".tar.gz");
        return tmpFileURI;
    }

    public static String generateTarURI(Path baseURI, String filename) {
        String tmpFileURI = new String(baseURI + Path.SEPARATOR + filename + ".tar.gz");
        return tmpFileURI;
    }

    public static String generateTarFileName(String name) {
        String tmpFileURI = new String(name + ".tar.gz");
        return tmpFileURI;
    }

    public static String generatePath(Path baseURI, String filename) {
        String path = new String(baseURI + Path.SEPARATOR + filename);
        return path;
    }

    public static String now() {
        Calendar cal = Calendar.getInstance();
        SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd hh:mm:ss");
        return sdf.format(cal.getTime());
    }

    public static double showTime(long time) {
        double result = (double) time / (double) 1000;
        return result;
    }

    public static void reworkMapRedWork(Task<? extends Serializable> task, boolean reworkMapredWork, HiveConf conf) throws SemanticException {
        if (reworkMapredWork && (task instanceof MapRedTask)) {
            try {
                MapredWork mapredWork = ((MapRedTask) task).getWork();
                Set<Class<? extends InputFormat>> reworkInputFormats = new HashSet<Class<? extends InputFormat>>();
                for (PartitionDesc part : mapredWork.getMapWork().getPathToPartitionInfo().values()) {
                    Class<? extends InputFormat> inputFormatCls = part.getInputFileFormatClass();
                    if (ReworkMapredInputFormat.class.isAssignableFrom(inputFormatCls)) {
                        reworkInputFormats.add(inputFormatCls);
                    }
                }
                if (reworkInputFormats.size() > 0) {
                    for (Class<? extends InputFormat> inputFormatCls : reworkInputFormats) {
                        ReworkMapredInputFormat inst = (ReworkMapredInputFormat) ReflectionUtils.newInstance(inputFormatCls, null);
                        inst.rework(conf, mapredWork);
                    }
                }
            } catch (IOException e) {
                throw new SemanticException(e);
            }
        }
    }

    public static class SQLCommand<T> {

        public T run(PreparedStatement stmt) throws SQLException {
            return null;
        }
    }

    public static <T> T executeWithRetry(SQLCommand<T> cmd, PreparedStatement stmt, int baseWindow, int maxRetries) throws SQLException {
        Random r = new Random();
        T result = null;
        for (int failures = 0; ; failures++) {
            try {
                result = cmd.run(stmt);
                return result;
            } catch (SQLTransientException e) {
                LOG.warn("Failure and retry #" + failures + " with exception " + e.getMessage());
                if (failures >= maxRetries) {
                    throw e;
                }
                long waitTime = getRandomWaitTime(baseWindow, failures, r);
                try {
                    Thread.sleep(waitTime);
                } catch (InterruptedException iex) {
                }
            } catch (SQLException e) {
                throw e;
            }
        }
    }

    public static Connection connectWithRetry(String connectionString, int waitWindow, int maxRetries) throws SQLException {
        Random r = new Random();
        for (int failures = 0; ; failures++) {
            try {
                Connection conn = DriverManager.getConnection(connectionString);
                return conn;
            } catch (SQLTransientException e) {
                if (failures >= maxRetries) {
                    LOG.error("Error during JDBC connection. " + e);
                    throw e;
                }
                long waitTime = Utilities.getRandomWaitTime(waitWindow, failures, r);
                try {
                    Thread.sleep(waitTime);
                } catch (InterruptedException e1) {
                }
            } catch (SQLException e) {
                throw e;
            }
        }
    }

    public static PreparedStatement prepareWithRetry(Connection conn, String stmt, int waitWindow, int maxRetries) throws SQLException {
        Random r = new Random();
        for (int failures = 0; ; failures++) {
            try {
                return conn.prepareStatement(stmt);
            } catch (SQLTransientException e) {
                if (failures >= maxRetries) {
                    LOG.error("Error preparing JDBC Statement " + stmt + " :" + e);
                    throw e;
                }
                long waitTime = Utilities.getRandomWaitTime(waitWindow, failures, r);
                try {
                    Thread.sleep(waitTime);
                } catch (InterruptedException e1) {
                }
            } catch (SQLException e) {
                throw e;
            }
        }
    }

    public static long getRandomWaitTime(int baseWindow, int failures, Random r) {
        return (long) (baseWindow * failures + baseWindow * (failures + 1) * r.nextDouble());
    }

    public static final char sqlEscapeChar = '\\';

    public static String escapeSqlLike(String key) {
        StringBuffer sb = new StringBuffer(key.length());
        for (char c : key.toCharArray()) {
            switch(c) {
                case '_':
                case '%':
                case sqlEscapeChar:
                    sb.append(sqlEscapeChar);
                default:
                    sb.append(c);
                    break;
            }
        }
        return sb.toString();
    }

    public static String formatMsecToStr(long msec) {
        long day = -1, hour = -1, minute = -1, second = -1;
        long ms = msec % 1000;
        long timeLeft = msec / 1000;
        if (timeLeft > 0) {
            second = timeLeft % 60;
            timeLeft /= 60;
            if (timeLeft > 0) {
                minute = timeLeft % 60;
                timeLeft /= 60;
                if (timeLeft > 0) {
                    hour = timeLeft % 24;
                    day = timeLeft / 24;
                }
            }
        }
        StringBuilder sb = new StringBuilder();
        if (day != -1) {
            sb.append(day + " days ");
        }
        if (hour != -1) {
            sb.append(hour + " hours ");
        }
        if (minute != -1) {
            sb.append(minute + " minutes ");
        }
        if (second != -1) {
            sb.append(second + " seconds ");
        }
        sb.append(ms + " msec");
        return sb.toString();
    }

    public static int estimateNumberOfReducers(HiveConf conf, ContentSummary inputSummary, MapWork work, boolean finalMapRed) throws IOException {
        long bytesPerReducer = conf.getLongVar(HiveConf.ConfVars.BYTESPERREDUCER);
        int maxReducers = conf.getIntVar(HiveConf.ConfVars.MAXREDUCERS);
        double samplePercentage = getHighestSamplePercentage(work);
        long totalInputFileSize = getTotalInputFileSize(inputSummary, work, samplePercentage);
        if (totalInputFileSize != inputSummary.getLength()) {
            LOG.info("BytesPerReducer=" + bytesPerReducer + " maxReducers=" + maxReducers + " estimated totalInputFileSize=" + totalInputFileSize);
        } else {
            LOG.info("BytesPerReducer=" + bytesPerReducer + " maxReducers=" + maxReducers + " totalInputFileSize=" + totalInputFileSize);
        }
        int reducers = (int) ((totalInputFileSize + bytesPerReducer - 1) / bytesPerReducer);
        reducers = Math.max(1, reducers);
        reducers = Math.min(maxReducers, reducers);
        if (conf.getBoolVar(HiveConf.ConfVars.HIVE_INFER_BUCKET_SORT_NUM_BUCKETS_POWER_TWO) && finalMapRed && !work.getBucketedColsByDirectory().isEmpty()) {
            int reducersLog = (int) (Math.log(reducers) / Math.log(2)) + 1;
            int reducersPowerTwo = (int) Math.pow(2, reducersLog);
            if (reducersPowerTwo / 2 == reducers) {
                return reducers;
            } else if (reducersPowerTwo > maxReducers) {
                reducers = reducersPowerTwo / 2;
            } else {
                reducers = reducersPowerTwo;
            }
        }
        return reducers;
    }

    public static long getTotalInputFileSize(ContentSummary inputSummary, MapWork work, double highestSamplePercentage) {
        long totalInputFileSize = inputSummary.getLength();
        if (work.getNameToSplitSample() == null || work.getNameToSplitSample().isEmpty()) {
            return totalInputFileSize;
        }
        if (highestSamplePercentage >= 0) {
            totalInputFileSize = Math.min((long) (totalInputFileSize * highestSamplePercentage / 100D), totalInputFileSize);
        }
        return totalInputFileSize;
    }

    public static long getTotalInputNumFiles(ContentSummary inputSummary, MapWork work, double highestSamplePercentage) {
        long totalInputNumFiles = inputSummary.getFileCount();
        if (work.getNameToSplitSample() == null || work.getNameToSplitSample().isEmpty()) {
            return totalInputNumFiles;
        }
        if (highestSamplePercentage >= 0) {
            totalInputNumFiles = Math.min((long) (totalInputNumFiles * highestSamplePercentage / 100D), totalInputNumFiles);
        }
        return totalInputNumFiles;
    }

    public static double getHighestSamplePercentage(MapWork work) {
        double highestSamplePercentage = 0;
        for (String alias : work.getAliasToWork().keySet()) {
            if (work.getNameToSplitSample().containsKey(alias)) {
                Double rate = work.getNameToSplitSample().get(alias).getPercent();
                if (rate != null && rate > highestSamplePercentage) {
                    highestSamplePercentage = rate;
                }
            } else {
                highestSamplePercentage = -1;
                break;
            }
        }
        return highestSamplePercentage;
    }

    public static List<Path> getInputPaths(JobConf job, MapWork work, String hiveScratchDir, Context ctx) throws Exception {
        int sequenceNumber = 0;
        Set<Path> pathsProcessed = new HashSet<Path>();
        List<Path> pathsToAdd = new LinkedList<Path>();
        for (String alias : work.getAliasToWork().keySet()) {
            LOG.info("Processing alias " + alias);
            Path path = null;
            for (String file : new LinkedList<String>(work.getPathToAliases().keySet())) {
                List<String> aliases = work.getPathToAliases().get(file);
                if (aliases.contains(alias)) {
                    path = new Path(file);
                    if (pathsProcessed.contains(path)) {
                        continue;
                    }
                    pathsProcessed.add(path);
                    LOG.info("Adding input file " + path);
                    if (isEmptyPath(job, path, ctx)) {
                        path = createDummyFileForEmptyPartition(path, job, work, hiveScratchDir, alias, sequenceNumber++);
                    }
                    pathsToAdd.add(path);
                }
            }
            if (path == null) {
                path = createDummyFileForEmptyTable(job, work, hiveScratchDir, alias, sequenceNumber++);
                pathsToAdd.add(path);
            }
        }
        return pathsToAdd;
    }

    @SuppressWarnings({ "rawtypes", "unchecked" })
    private static Path createEmptyFile(String hiveScratchDir, Class<? extends HiveOutputFormat> outFileFormat, JobConf job, int sequenceNumber, Properties props, boolean dummyRow) throws IOException, InstantiationException, IllegalAccessException {
        String newDir = hiveScratchDir + File.separator + sequenceNumber;
        Path newPath = new Path(newDir);
        FileSystem fs = newPath.getFileSystem(job);
        fs.mkdirs(newPath);
        newPath = fs.makeQualified(newPath);
        String newFile = newDir + File.separator + "emptyFile";
        Path newFilePath = new Path(newFile);
        String onefile = newPath.toString();
        RecordWriter recWriter = outFileFormat.newInstance().getHiveRecordWriter(job, newFilePath, Text.class, false, props, null);
        if (dummyRow) {
            recWriter.write(new Text("empty"));
        }
        recWriter.close(false);
        return newPath;
    }

    @SuppressWarnings("rawtypes")
    private static Path createDummyFileForEmptyPartition(Path path, JobConf job, MapWork work, String hiveScratchDir, String alias, int sequenceNumber) throws IOException, InstantiationException, IllegalAccessException {
        String strPath = path.toString();
        PartitionDesc partDesc = work.getPathToPartitionInfo().get(strPath);
        boolean nonNative = partDesc.getTableDesc().isNonNative();
        boolean oneRow = partDesc.getInputFileFormatClass() == OneNullRowInputFormat.class;
        Properties props = partDesc.getProperties();
        Class<? extends HiveOutputFormat> outFileFormat = partDesc.getOutputFileFormatClass();
        if (nonNative) {
            return path;
        }
        Path newPath = createEmptyFile(hiveScratchDir, outFileFormat, job, sequenceNumber, props, oneRow);
        LOG.info("Changed input file to " + newPath);
        String strNewPath = newPath.toString();
        LinkedHashMap<String, ArrayList<String>> pathToAliases = work.getPathToAliases();
        pathToAliases.put(strNewPath, pathToAliases.get(strPath));
        pathToAliases.remove(strPath);
        work.setPathToAliases(pathToAliases);
        LinkedHashMap<String, PartitionDesc> pathToPartitionInfo = work.getPathToPartitionInfo();
        pathToPartitionInfo.put(strNewPath, pathToPartitionInfo.get(strPath));
        pathToPartitionInfo.remove(strPath);
        work.setPathToPartitionInfo(pathToPartitionInfo);
        return newPath;
    }

    @SuppressWarnings("rawtypes")
    private static Path createDummyFileForEmptyTable(JobConf job, MapWork work, String hiveScratchDir, String alias, int sequenceNumber) throws IOException, InstantiationException, IllegalAccessException {
        TableDesc tableDesc = work.getAliasToPartnInfo().get(alias).getTableDesc();
        Properties props = tableDesc.getProperties();
        boolean nonNative = tableDesc.isNonNative();
        Class<? extends HiveOutputFormat> outFileFormat = tableDesc.getOutputFileFormatClass();
        if (nonNative) {
            return null;
        }
        Path newPath = createEmptyFile(hiveScratchDir, outFileFormat, job, sequenceNumber, props, false);
        LOG.info("Changed input file to " + newPath.toString());
        LinkedHashMap<String, ArrayList<String>> pathToAliases = work.getPathToAliases();
        ArrayList<String> newList = new ArrayList<String>();
        newList.add(alias);
        pathToAliases.put(newPath.toUri().toString(), newList);
        work.setPathToAliases(pathToAliases);
        LinkedHashMap<String, PartitionDesc> pathToPartitionInfo = work.getPathToPartitionInfo();
        PartitionDesc pDesc = work.getAliasToPartnInfo().get(alias).clone();
        pathToPartitionInfo.put(newPath.toUri().toString(), pDesc);
        work.setPathToPartitionInfo(pathToPartitionInfo);
        return newPath;
    }

    public static void setInputPaths(JobConf job, List<Path> pathsToAdd) {
        Path[] addedPaths = FileInputFormat.getInputPaths(job);
        if (addedPaths == null) {
            addedPaths = new Path[0];
        }
        Path[] combined = new Path[addedPaths.length + pathsToAdd.size()];
        System.arraycopy(addedPaths, 0, combined, 0, addedPaths.length);
        int i = 0;
        for (Path p : pathsToAdd) {
            combined[addedPaths.length + (i++)] = p;
        }
        FileInputFormat.setInputPaths(job, combined);
    }

    public static void setInputAttributes(Configuration conf, MapWork mWork) {
        if (mWork.getInputformat() != null) {
            HiveConf.setVar(conf, HiveConf.ConfVars.HIVEINPUTFORMAT, mWork.getInputformat());
        }
        if (mWork.getIndexIntermediateFile() != null) {
            conf.set("hive.index.compact.file", mWork.getIndexIntermediateFile());
            conf.set("hive.index.blockfilter.file", mWork.getIndexIntermediateFile());
        }
        conf.setBoolean("hive.input.format.sorted", mWork.isInputFormatSorted());
    }

    public static void createTmpDirs(Configuration conf, MapWork mWork) throws IOException {
        Map<String, ArrayList<String>> pa = mWork.getPathToAliases();
        if (pa != null) {
            List<Operator<? extends OperatorDesc>> ops = new ArrayList<Operator<? extends OperatorDesc>>();
            for (List<String> ls : pa.values()) {
                for (String a : ls) {
                    ops.add(mWork.getAliasToWork().get(a));
                }
            }
            createTmpDirs(conf, ops);
        }
    }

    @SuppressWarnings("unchecked")
    public static void createTmpDirs(Configuration conf, ReduceWork rWork) throws IOException {
        if (rWork == null) {
            return;
        }
        List<Operator<? extends OperatorDesc>> ops = new LinkedList<Operator<? extends OperatorDesc>>();
        ops.add(rWork.getReducer());
        createTmpDirs(conf, ops);
    }

    private static void createTmpDirs(Configuration conf, List<Operator<? extends OperatorDesc>> ops) throws IOException {
        while (!ops.isEmpty()) {
            Operator<? extends OperatorDesc> op = ops.remove(0);
            if (op instanceof FileSinkOperator) {
                FileSinkDesc fdesc = ((FileSinkOperator) op).getConf();
                String tempDir = fdesc.getDirName();
                if (tempDir != null) {
                    Path tempPath = Utilities.toTempPath(new Path(tempDir));
                    FileSystem fs = tempPath.getFileSystem(conf);
                    fs.mkdirs(tempPath);
                }
            }
            if (op.getChildOperators() != null) {
                ops.addAll(op.getChildOperators());
            }
        }
    }
}